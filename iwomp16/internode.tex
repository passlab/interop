Hybrid parallel programming in the form of internode+intranode, e.g. MPI+X model are widely used  
for HPC, which is the typical example for the Concurrent class. 
This hybrid approach reflects the two-level hierarchy of hardware parallelism 
in current HPC systems, in which network connects many highly parallel nodes.
Interoperability between inter- and intra-node APIs such as MPI+OpenMP 
has long been a productivity and composability goal within
the HPC community. We however still have not agreed on a 
standard solution from either of the two communities. 


